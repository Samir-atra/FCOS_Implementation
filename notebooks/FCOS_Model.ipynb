{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Samir-atra/FCOS_Implementation/blob/main/model/FCOS_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Install and import libraries","metadata":{}},{"cell_type":"code","source":"# imports\n# !pip install -U pip\n# !pip install -U six numpy wheel packaging\n# !pip install -U keras_preprocessing --no-deps\n# !pip install opencv-python\n# !pip install tf-models-official\n\n# import tensorflow_models as tfm\nimport tqdm\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport os\nimport cv2\nimport re\nfrom operator import itemgetter\n","metadata":{"id":"is_N0C5RFKs5","outputId":"ad807ce4-be83-4afe-85f1-4b1ebb966c91","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Download COCO dataset","metadata":{}},{"cell_type":"code","source":"# download the dataset \n\n!wget http://images.cocodataset.org/zips/train2014.zip\n!wget http://images.cocodataset.org/zips/val2014.zip\n!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n!wget http://images.cocodataset.org/zips/test2014.zip\n!wget http://images.cocodataset.org/annotations/image_info_test2014.zip\n","metadata":{"id":"FMKgsJTSFKs6","outputId":"c5219fa7-b7d5-4dde-863e-dc1441e5983e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"8ZocsC7SUEG1","outputId":"79d6fd5d-ca9b-409d-eef2-3db6271c9ef9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unzip files","metadata":{}},{"cell_type":"code","source":"# unzip the dataset files\n\n!pip install unzip\n!unzip \"/kaggle/working/annotations_trainval2014.zip\"\n!unzip \"/kaggle/working/image_info_test2014.zip\"\n!unzip \"/kaggle/working/test2014.zip\"\n!unzip \"/kaggle/working/train2014.zip\"\n!unzip \"/kaggle/working/val2014.zip\"\n","metadata":{"id":"2vMFUP6UFKs6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Datasets paths","metadata":{}},{"cell_type":"code","source":"# dataset paths\n\n# for colab\ntrain_imgs_path = \"/content/drive/MyDrive/COCO/train2014/\"\nannotations = \"/content/drive/MyDrive/COCO/annotations/\"\n\n\n#for library\n# train_imgs_path = \"C:/Users/1845718/Documents/FCOS_Implementation/train2014/train2014/\"\n# val_imgs_path = \"C:/Users/1845718/Documents/FCOS_Implementation/val2014/val2014/\"\n# test_imgs_path = \"C:/Users/1845718/Documents/FCOS_Implementation/test2014/test2014/\"\n# annotations = \"C:/Users/1845718/Documents/FCOS_Implementation/annotations_trainval2014/annotations/\"\n\n# for laptop\n# train_imgs_path = \"/home/samer/Desktop/Beedoo/FCOS/FCOS_Implementation/COCO2014/train2014/\"\n# val_imgs_path = \"/home/samer/Desktop/Beedoo/FCOS/FCOS_Implementation/COCO2014/val2014/\"\n# test_imgs_path = \"/home/samer/Desktop/Beedoo/FCOS/FCOS_Implementation/COCO2014/test2014/\"\n# annotations = \"/home/samer/Desktop/Beedoo/FCOS/FCOS_Implementation/COCO2014/annotations/\"","metadata":{"id":"ZINKK2KsFKs6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# images_dict = dict()\n# image_list = []\n# counter = 0\n\n# # load the images\n# for file in sorted(os.listdir(train_imgs_path)):\n#     if counter < 600:\n#         image_path = os.path.join(train_imgs_path, file)\n#         image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n#         image = cv2.resize(image, (800, 1024))\n#         image = np.array(image)\n#         image = image.astype('float32')\n#         image /= 255\n#         if len(image.shape) < 3:\n#             continue\n#         # plt.imshow(image)\n#         # plt.show()\n#         file_key = str(file).split('.')[0]\n#         imageid = re.findall(\"[0-9]+\", file_key)\n#         imageid = str(imageid[1]).lstrip(\"_0\")\n#         images_dict[f'{imageid}'] = image           # create a dictionary of the image titles and themselves\n#         image_list.append(image)\n#         counter += 1\n#     else:\n#         break","metadata":{"id":"NHgfVtDRFKs7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Util function","metadata":{}},{"cell_type":"code","source":"def concat(list1, list2):\n    \"\"\"a concatination function for the COCO dataset bounding box labels\"\"\"\n    if isinstance(list1[0], list) and isinstance(list2, list):\n        alist = list1\n        alist.append(list2)\n    elif isinstance(list2, int):\n        alist = [list1]\n        alist.append(list2)\n    return alist\n\n# check if need to put all the boxes of a single image in a list instead of keeping them separate.\n# concat([1.08, 187.69, 611.59, 285.84],51)","metadata":{"id":"IE3w-4tDFKs7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analyze the dataset format","metadata":{}},{"cell_type":"code","source":"labels_list = []\nbounding_boxes = []\ncounter = 0\ndont_care = [0,0,0,0,0,0,0,0,0]\nannotation_path = os.path.join(annotations, \"instances_train2014.json\")\nboxes = dict()\ncounter = 0\n\nannotation_path = \"/home/samer/Desktop/Beedoo/FCOS/FCOS_Implementation/COCO2014/annotations/instances_train2014.json\"\n\ndef calculate_distances():\n    \"\"\"find the distance of the \"\"\"\n\n\n\ndef get_regression_target():\n    \"\"\"creates the regression target for the FCOS model\"\"\"\n    \n    boxes = {}\n    counter = 0\n    \n    with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n        json_obj = json.load(f)\n        sorted_imageids = sorted(json_obj[\"annotations\"], key=itemgetter(\"image_id\"))\n\n        for line in sorted_imageids:  \n                if counter < 500:                  # due to limited computation had to limit the number of boxes to be loaded\n                    if not boxes.get(line[\"image_id\"]):\n                        boxes[line[\"image_id\"]] = [concat(line[\"bbox\"], )]\n                        \n                    elif boxes.get(line[\"image_id\"]):\n                        boxes[line[\"image_id\"]] = concat(boxes[line[\"image_id\"]],concat(line[\"bbox\"], line[\"category_id\"]),)\n                    \n                    counter += 1\n\n    return boxes\n","metadata":{"id":"Y2rgYKZxFKs7","outputId":"47b16443-44de-4d03-9056-071dd6d65f5f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the datasets into memory","metadata":{}},{"cell_type":"code","source":"\nclass DataLoader:\n    \"\"\"loading the COCO dataset images and bounding boxes as labels,\n    contains two methods on for each functionality\"\"\"\n    def __init__(self, train_images_path, annotations_path):\n        self.train_images_path = train_images_path\n        self.annotations_path = annotations_path\n\n    def load_images(self, data_path):\n        \"\"\"loads the images of the dataset into a list\"\"\"\n        images_dict = {}\n        image_list = []\n        counter = 0\n        # load the images\n        for file in sorted(tf.io.gfile.listdir(train_imgs_path)):\n            if counter < 100:\n                image_path = tf.io.gfile.join(train_imgs_path, file)\n                image = tf.io.read_file(image_path)\n                image = tf.io.decode_image(image, dtype=tf.dtypes.float32)\n                image = tf.image.resize(\n                    image, (800, 1024), method=\"bilinear\", preserve_aspect_ratio=False\n                )\n                if len(image.shape) < 3:\n                    continue\n                # plt.imshow(image)\n                # plt.show()\n                file_key = str(file).split(\".\")[0]\n                imageid = re.findall(\"[0-9]+\", file_key)\n                imageid = str(imageid[1]).lstrip(\"_0\")\n                images_dict[f\"{imageid}\"] = (\n                    image  # create a dictionary of the image titles and themselves\n                )\n                plt.imshow(image)\n                plt.show()\n                # print(image)\n                image_list.append(image)\n                counter += 1\n            else:\n                break\n        return image_list\n\n\n    def load_labels(self, file_path):\n        \"\"\"creates labels lists for each image consisting of the image id and\n        category id and the bounding box for each object in the image\"\"\"\n\n        labels_list = []\n        bounding_boxes = []\n        counter = 0\n        dont_care = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n        annotation_path = tf.io.gfile.join(annotations, \"instances_train2014.json\")\n        boxes = {}\n        counter = 0\n\n        with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n            json_obj = json.load(f)\n            sorted_imageids = sorted(\n                json_obj[\"annotations\"], key=itemgetter(\"image_id\")\n            )\n            # draw the bounding boxes\n            for (\n                line\n            ) in (\n                sorted_imageids\n            ):  # iterate over the lines of the label file, where each line is a box with it's label\n                # print(line)\n                if counter < 500:                  # due to limited computation had to limit the number of boxes to be loaded\n                    if not boxes.get(line[\"image_id\"]):\n                        boxes[line[\"image_id\"]] = [\n                            concat(line[\"bbox\"], line[\"category_id\"])\n                        ]\n                    elif boxes.get(line[\"image_id\"]):\n                        boxes[line[\"image_id\"]] = concat(\n                            boxes[line[\"image_id\"]],\n                            concat(line[\"bbox\"], line[\"category_id\"]),\n                        )\n                    counter += 1\n\n        return boxes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet backbone model","metadata":{}},{"cell_type":"code","source":"# model backbone\n# FCOS uses ResNeXt not his one.\n\ndef backbone():\n    \"\"\"\n    imaplementation of the ResNet backbone model pre-trained \n    on the ImageNet dataset.\n    \n    Returns:\n        c3: the output of the third resnet block\n        c4: the output of the forth resnet block\n        c5: the output of the fifth resent block\n    \"\"\"\n    \n    resnet = tf.keras.applications.ResNet50(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(800, 1024, 3),\n            classes=80,\n        )\n\n    c3, c4, c5 = resnet.get_layer(\"conv3_block4_out\").output, resnet.get_layer(\"conv4_block6_out\").output, resnet.get_layer(\"conv5_block3_out\").output\n    \n    return c3, c4, c5\n\nexp_config = tfm.core.exp_factory.get_exp_config('resnet_imagenet')\nexp_config.task.model.num_classes = 8\nexp_config.task.model.input_size = list(1024,800,3)\nexp_config.task.model.backbone.resnet.model_id = 50\nbatch_size = 16\n","metadata":{"id":"OEVPY-zuFKs7","outputId":"9820b22e-4e36-4bb7-ae63-e7f40f00529e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FPN backbone Model","metadata":{}},{"cell_type":"code","source":"\nclass FPN(tf.keras.layers.Layer):\n    \n    def __init__(self):\n        super(FPN, self).__init__()\n        self.conv3_1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv4_1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv5_1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv3_3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv4_3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv5_3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv6_3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.conv7_3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.upsample = tf.keras.layers.UpSampling2D(2)\n\n    def call(self, c3, c4, c5):\n        \"\"\"\n        the feature pyramid network implementation\n            \n        Args:\n            c3 : the third resnet block output\n            c4 : the fourth resnet block output\n            c5 : the fifth resnet block outout \n\n        Returns:\n            [p3_out, p4_out, p5_out, p6_out, p7_out] (list): list of the outputs of \n            the third forth fifth sixth seventh layers of the feature pyramid network\n        \"\"\"\n        # there are changes to be done here, adding GN\n        p3_out = self.conv3_1(c3)\n        p4_out = self.conv4_1(c4)\n        p5_out = self.conv5_1(c5)\n        p4_out = p4_out + self.upsample(p5_out)\n        p3_out = p3_out + self.upsample(p4_out)\n        p3_out = self.conv3_3(p3_out)\n        p4_out = self.conv4_3(p4_out)\n        p5_out = self.conv5_3(p5_out)\n        p6_out = self.conv6_3(p5_out)\n        p7_out = self.conv7_3(tf.nn.relu(p6_out))\n\n        return [p3_out, p4_out, p5_out, p6_out, p7_out]\n","metadata":{"id":"bwCo11eYFKs8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Heads function","metadata":{}},{"cell_type":"code","source":"\ndef head(output_classes, bias):\n    \"\"\"the output head for the FCOS model from the FPN\n\n    Args:\n        output_classes (int): number of classes in the output layer\n        bias: the initial bias value\n    Output:\n        model (keras.model): the head model\n    \"\"\"\n    kernel_initial = tf.initializers.RandomNormal(0.0, 0.01)\n    model = tf.keras.Sequential(\n        [\n            tf.keras.Input(shape=[None, None, 256]),\n            tf.keras.layers.Conv2D(\n                256, 3, padding=\"same\", kernel_initializer=kernel_initial\n            ),\n            tf.keras.layers.ReLU(),\n            # tf.keras.layers.GroupNormalization(),\n            tf.keras.layers.Conv2D(\n                256, 3, padding=\"same\", kernel_initializer=kernel_initial\n            ),\n            tf.keras.layers.ReLU(),\n            # tf.keras.layers.GroupNormalization(),\n            tf.keras.layers.Conv2D(\n                256, 3, padding=\"same\", kernel_initializer=kernel_initial\n            ),\n            tf.keras.layers.ReLU(),\n            # tf.keras.layers.GroupNormalization(),\n            tf.keras.layers.Conv2D(\n                256, 3, padding=\"same\", kernel_initializer=kernel_initial\n            ),\n            tf.keras.layers.ReLU(),\n            # tf.keras.layers.GroupNormalization(),\n            tf.keras.layers.Conv2D(\n                output_classes,\n                3,\n                1,\n                padding=\"same\",\n                kernel_initializer=kernel_initial,\n                bias_initializer=bias,\n            ),\n        ]\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss functions","metadata":{}},{"cell_type":"code","source":"\nclass IOULoss(tf.keras.Loss):\n    \"\"\"Intersection Over Union (IOU) loss function\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def call(self, pred, g_label):\n        \"\"\"the method includes the implementation of the IOU loss function\"\"\"\n        if g_label != 0:\n            pred_left = pred[0]\n            pred_top = pred[1]\n            pred_right = pred[2]\n            pred_bottom = pred[3]\n\n            g_label_left = g_label[0]\n            g_label_top = g_label[1]\n            g_label_right = g_label[2]\n            g_label_bottom = g_label[3]\n\n            target_area = (g_label_left + g_label_right) * (\n                g_label_top + g_label_bottom\n            )\n            pred_area = (pred_left + pred_right) * (pred_top + pred_bottom)\n\n            w_intersect = tf.math.minimum(pred_left, g_label_left) + tf.math.minimum(\n                pred_right, g_label_right\n            )\n            g_w_intersect = tf.math.maximum(pred_left, g_label_left) + tf.math.maximum(\n                pred_right, g_label_right\n            )\n            h_intersect = tf.math.minimum(\n                pred_bottom, g_label_bottom\n            ) + tf.math.minimum(pred_top, g_label_top)\n            g_h_intersect = tf.math.maximum(\n                pred_bottom, g_label_bottom\n            ) + tf.math.maximum(pred_top, g_label_top)\n            ac_union = g_w_intersect * g_h_intersect + 1e-7\n            area_intersect = w_intersect * h_intersect\n            area_union = target_area + pred_area - area_intersect\n\n            ious = (area_intersect + 1.0) / (\n                area_union + 1.0\n            )  # need to check what these are and the next line and compare pytorch to research paper.\n            gious = ious - (ac_union - area_union) / ac_union\n\n            loss = -tf.math.log(ious)\n\n            return loss\n\n        else:\n            loss = 0\n            return loss\n\n\nclass FcosLoss(tf.keras.Loss):\n    \"\"\"implementation of the FCOS loss function\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def call(self, pred, g_label):\n        \"\"\"\n        the method includes the implementation of the FCOS loss function\n        Args:\n            pred : the prediction of the model\n            g_label : the ground truth label of the model\n        Returns:\n            the loss value\n        \"\"\"\n        focal = tf.keras.losses.CategoricalFocalCrossentropy()\n        iouloss = IOULoss()\n        bce = tf.keras.losses.BinaryCrossentropy()\n        \n        return focal(pred, g_label) , iouloss(pred, g_label) , bce(pred, g_label)\n","metadata":{"id":"o_UTEuXnWX9w","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model FCOS","metadata":{}},{"cell_type":"code","source":"\nclass FCOS(tf.keras.Model):\n    \"\"\"\n    implementation for the FCOS model using the \n    ResNet50 as a backbone\n    \"\"\"\n    def __init__(self):\n        super(FCOS, self).__init__()\n        self.backbone = backbone()\n        self.pyramid = FPN()\n        self.bias = tf.keras.initializers.Constant(-tf.math.log((1 - 0.01) / 0.01))\n        self.class_num = 80\n        self.classification_head = head(self.class_num, self.bias)\n        self.centerness_head = head(1, self.bias)\n        self.box_head = head(4, \"zero\")\n        \n    def call(self):\n        \"\"\"\n        the full fcos model assembled parts \n        \n        Returns:\n            list of all the outputs of the model heads \n            for one training example\n        \"\"\"\n        c3, c4, c5 = self.backbone()\n        p3_out, p4_out, p5_out, p6_out, p7_out = self.pyramid(c3, c4, c5)\n        \n        classifier_out = []\n        box_out = []\n        centerness_out = []\n        \n        for layer in [p3_out, p4_out, p5_out, p6_out, p7_out]:\n            classifier_out.append(self.classification_head(layer))\n            centerness_out.append(self.centerness_head(layer))\n            box_out.append(self.box_head(layer))\n        \n        classifier_out = tf.concat(classifier_out, axis = 1, name = \"classifier\")\n        centerness_out = tf.concat(centerness_out, axis = 1, name = \"centerness\")\n        box_out = tf.concat(box_out, axis = 1, name = \"box\")\n        \n        return tf.concat([classifier_out, centerness_out, box_out], axis = -1)\n\n\n\n# loss function\nfocal = tf.keras.losses.CategoricalFocalCrossentropy(\n    alpha = 0.25,\n    gamma = 2.0,\n)\niouloss = IOULoss()\nbce = tf.keras.losses.BinaryCrossentropy()\n\nmodel = FCOS()\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01,\n              weight_decay = 0.0001,\n              momentum = 0.9),\n              loss = {'classifier': focal, 'box': iouloss, 'centerness': bce},\n              metrics = ['precision'])\n\n\ndef schedule(epoch, lr):\n  if epoch == 60000 or epoch == 80000:\n    lr = lr / 10\n    return lr\n\nsched = tf.keras.callbacks.LearningRateScheduler(\n    schedule\n)\n\nmodel.fit(\n          epochs = 90000,\n          batch_size = 16,\n          callbacks = [sched],\n          )\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}